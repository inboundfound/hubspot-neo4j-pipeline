#!/usr/bin/env python3
"""Main pipeline orchestrator"""

import json
import time
from datetime import datetime
import os

from extractors.contacts import ContactsExtractor
from extractors.companies import CompaniesExtractor
from extractors.deals import DealsExtractor
from extractors.engagements import EngagementsExtractor
from extractors.email_events import EmailEventsExtractor
from extractors.users import UsersExtractor
from extractors.form_submissions import FormSubmissionsExtractor
from transformers.graph_transformer import GraphTransformer
from loaders.temporal_loader import TemporalLoader
from utils.logger import setup_logger

def run_pipeline():
    """Run the complete HubSpot to Neo4j pipeline"""
    logger = setup_logger('Pipeline')
    logger.info("üöÄ Starting HubSpot to Neo4j Pipeline")
    
    start_time = time.time()
    
    # Create output directory for raw data
    os.makedirs('data/raw', exist_ok=True)
    
    try:
        # Step 1: Extract data from HubSpot
        logger.info("\n" + "="*50)
        logger.info("STEP 1: EXTRACTING DATA FROM HUBSPOT")
        logger.info("="*50)
        
        all_data = {}
        # with open('data/raw/contacts.json', 'r') as f:
        #     all_data['contacts'] = json.load(f)
        # with open('data/raw/companies.json', 'r') as f:
        #     all_data['companies'] = json.load(f)
        # with open('data/raw/deals.json', 'r') as f:
        #     all_data['deals'] = json.load(f)
        # with open('data/raw/engagements.json', 'r') as f:
        #     all_data['engagements'] = json.load(f)
        # with open('data/raw/email_events.json', 'r') as f:
        #     all_data['email_events'] = json.load(f)
        # with open('data/raw/users.json', 'r') as f:
        #     all_data['users'] = json.load(f)
        # with open('data/raw/form_submissions.json', 'r') as f:
        #     all_data['form_submissions'] = json.load(f)
        
        # Extract contacts
        logger.info("\nüìã Extracting Contacts...")
        contacts_extractor = ContactsExtractor()
        all_data['contacts'] = contacts_extractor.extract_all()
        contacts_extractor.save_to_json('data/raw/contacts.json')
        
        # Extract companies
        logger.info("\nüè¢ Extracting Companies...")
        companies_extractor = CompaniesExtractor()
        all_data['companies'] = companies_extractor.extract_all()
        companies_extractor.save_to_json('data/raw/companies.json')
        
        # Extract deals
        logger.info("\nüí∞ Extracting Deals...")
        deals_extractor = DealsExtractor()
        all_data['deals'] = deals_extractor.extract_all()
        deals_extractor.save_to_json('data/raw/deals.json')
        
        # Extract engagements
        logger.info("\nüìÖ Extracting Engagements...")
        engagements_extractor = EngagementsExtractor()
        all_data['engagements'] = engagements_extractor.extract_all()
        engagements_extractor.save_to_json('data/raw/engagements.json')
        
        # Extract email events
        logger.info("\nüìß Extracting Email Events...")
        email_events_extractor = EmailEventsExtractor()
        all_data['email_events'] = email_events_extractor.extract_all()
        email_events_extractor.save_to_json('data/raw/email_events.json')

        # Extract users/owners
        logger.info("\nüë• Extracting Users/Owners...")
        users_extractor = UsersExtractor()
        all_data['users'] = users_extractor.extract_all()
        users_extractor.save_to_json('data/raw/users.json')

        # Extract form submissions
        logger.info("\nüìù Extracting Form Submissions...")
        form_submissions_extractor = FormSubmissionsExtractor()
        all_data['form_submissions'] = form_submissions_extractor.extract_all()
        form_submissions_extractor.save_to_json('data/raw/form_submissions.json')

        # Step 2: Transform data
        logger.info("\n" + "="*50)
        logger.info("STEP 2: TRANSFORMING DATA")
        logger.info("="*50)
        
        transformer = GraphTransformer()
        nodes, relationships = transformer.transform_all(all_data)
        
        # Save transformed data
        os.makedirs('data/transformed', exist_ok=True)
        with open('data/transformed/nodes.json', 'w') as f:
            json.dump(nodes, f, indent=2, default=str)
        with open('data/transformed/relationships.json', 'w') as f:
            json.dump(relationships, f, indent=2, default=str)
        
        # Step 3: Load into Neo4j with temporal tracking
        logger.info("\n" + "="*50)
        logger.info("STEP 3: LOADING INTO NEO4J (WITH TEMPORAL TRACKING)")
        logger.info("="*50)
        
        loader = TemporalLoader()
        loader.load_with_history(nodes, relationships)
        
        # Verify the load
        counts = loader.verify_load()
        
        # Clean up
        loader.close()
        
        # Summary
        elapsed_time = time.time() - start_time
        logger.info("\n" + "="*50)
        logger.info("‚úÖ PIPELINE COMPLETE")
        logger.info("="*50)
        logger.info(f"\nExecution time: {elapsed_time:.2f} seconds")
        logger.info(f"Extraction summary:")
        for key, data in all_data.items():
            logger.info(f"  - {key}: {len(data)} records")
        logger.info(f"\nGraph summary:")
        logger.info(f"  - Nodes: {sum(len(n) for n in nodes.values())}")
        logger.info(f"  - Relationships: {len(relationships)}")
        logger.info(f"\nNeo4j summary (current state):")
        
        # Group counts by entity type
        entity_types = ['HUBSPOT_Contact', 'HUBSPOT_Company', 'HUBSPOT_Deal', 
                       'HUBSPOT_Activity', 'HUBSPOT_User', 'HUBSPOT_EmailCampaign',
                       'HUBSPOT_WebPage', 'HUBSPOT_EmailOpenEvent', 'HUBSPOT_EmailClickEvent',
                       'HUBSPOT_FormSubmission', 'HUBSPOT_PageVisit']
        
        for entity_type in entity_types:
            if entity_type in counts and counts[entity_type] > 0:
                logger.info(f"  - {entity_type}: {counts[entity_type]}")
        
        logger.info(f"  - Relationships: {counts.get('relationships', 0)}")
        logger.info(f"  - Relationship changes tracked: {counts.get('relationship_changes', 0)}")
        logger.info(f"  - Deleted nodes: {counts.get('deleted_nodes', 0)}")
        
        # Show history counts
        history_total = sum(counts.get(f'{et}_HISTORY', 0) for et in 
                           ['HUBSPOT_Contact', 'HUBSPOT_Company', 'HUBSPOT_Deal', 
                            'HUBSPOT_Activity', 'HUBSPOT_User'])
        if history_total > 0:
            logger.info(f"  - Historical versions: {history_total}")
        
        # Example queries
        logger.info("\n" + "="*50)
        logger.info("EXAMPLE NEO4J QUERIES")
        logger.info("="*50)
        logger.info("""
1. Find contacts with the most email engagement:
   MATCH (c:Contact)
   WHERE c.total_email_opens > 0
   RETURN c.email, c.total_email_opens, c.total_email_clicks
   ORDER BY c.total_email_opens DESC
   LIMIT 10

2. Show deals by company:
   MATCH (d:Deal)-[:BELONGS_TO]->(c:Company)
   RETURN c.name, collect(d.name) as deals, sum(d.amount) as total_value

3. Find most active contacts by activity count:
   MATCH (c:Contact)<-[:INVOLVES]-(a:Activity)
   RETURN c.email, count(a) as activity_count
   ORDER BY activity_count DESC
   LIMIT 10

4. Email campaign effectiveness:
   MATCH (c:Contact)-[o:OPENED]->(e:EmailCampaign)
   WITH e, count(DISTINCT c) as opens
   MATCH (c:Contact)-[cl:CLICKED]->(e)
   WITH e, opens, count(DISTINCT c) as clicks
   RETURN e.name, opens, clicks, (clicks * 100.0 / opens) as click_rate
   ORDER BY opens DESC
        """)
        
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    # Test connections first
    print("\nüîß Testing connections before running pipeline...")
    from tests.test_connection import test_hubspot_connection, test_neo4j_connection
    
    if test_hubspot_connection() and test_neo4j_connection():
        print("\n‚úÖ Connection tests passed. Starting pipeline...\n")
        time.sleep(2)
        run_pipeline()
    else:
        print("\n‚ùå Connection tests failed. Please fix configuration before running pipeline.")
